---
layout: post
title: Deep Compression Reading List
excerpt: Deep neural network compression literature research organized chronologically.
categories:
  - Deep Neural Networks
comments: true
pinned: true
published: true
---
## Format

[**paper-name**].[authors].[publisher].[year] - [_reader_]
{: .notice}

Distilling the knowledge in a neural network. Hinton et al. arXiv preprint arXiv:1503.02531. 2015 - Kaushalya
{: .notice}

> We can add notices containing github link or other materials provided by the authors like this.

## Approaches

To compress a neural network, currently there are several approaches:

1. Stochastic Block Models.
2. Tensor Factorization (incl. all decompression techniques).
3. Determinantal Point Processes and Submodularity Models.
4. Network Distillation and Prunning (incl. fine tuning, etc.)
5. Graph Embedding (incl. graph summarization).
6. Small or new architectures.

## Mar 2017

**Soft Weight-Sharing for Neural Network Compression**. Ullrich et al. ICLR. 2017 - _Choong, Kaushalya_
{: .notice}

**Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding**. Han et al. ICLR. 2016 - _Choong, Kaushalya_
{: .notice}

**Learning both Weights and Connections for Efficient
Neural Networks**. Han et al. NIPS. 2015 - _Choong, Kaushalya_
{: .notice}

**Sparse Convolutional Neural Networks**. Liu et al. CVPR. 2015 - _Hoang_
{: .notice}

## Feb 2017

## Jan 2017

**Distilling the knowledge in a neural network**. Hinton et al. arXiv preprint arXiv:1503.02531. 2015 - _Kaushalya_
{: .notice}
