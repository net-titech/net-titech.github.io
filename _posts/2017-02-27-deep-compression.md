---
layout: post
title: Deep Compression Reading List
excerpt: "Deep neural network compression literature research organized chronologically."
categories: [Deep Neural Networks]
comments: true
pinned: true
---
## Format

[**paper-name**].[authors].[publisher].[year] - [_reader_]
{: .notice}

Distilling the knowledge in a neural network. Hinton et al. arXiv preprint arXiv:1503.02531. 2015 - Kaushalya
{: .notice}

> We can add notices containing github link or other materials provided by the authors like this.

## Approaches

To compress a neural network, currently there are several approaches:

1. Stochastic Block Models.
2. Tensor Factorization (incl. all decompression techniques).
3. Determinantal Point Processes and Submodularity Models.
4. Network Distillation (incl. prunning, fine tuning, etc.)
5. Graph Embedding (incl. graph summarization).
6. Small or new architectures.

## Mar 2017

**Sparse Convolutional Neural Networks**. Liu et al. CVPR. 2015 - _Hoang_
{: .notice}

## Feb 2017

## Jan 2017

**Distilling the knowledge in a neural network**. Hinton et al. arXiv preprint arXiv:1503.02531. 2015 - _Kaushalya_
{: .notice}


