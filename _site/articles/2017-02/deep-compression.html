<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Deep Compression Reading List</title>
  <meta name="description" content="Deep neural network compression literature research organized chronologically.">

  <!-- CSS files -->
  <link rel="stylesheet" href="/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/main.css">

  <link rel="canonical" href="/articles/2017-02/deep-compression">
  <link rel="alternate" type="application/rss+xml" title="Complex Network Research Group (Murata Lab)" href="/feed.xml" />

  <!-- Icons -->
  <!-- 16x16 -->
  <link rel="shortcut icon" href="/favicon.ico">
  <!-- 32x32 -->
  <link rel="shortcut icon" href="/favicon.png">
</head>


<body>
  <div class="row">
    <div class="col s12 m3">
      <div class="table cover">
        

<div class="cover-card table-cell table-middle">
  
  <img src="/img/net-logo.png" alt="" class="avatar">
  
  <a href="/" class="author_name">Murata Laboratory</a>
  <span class="author_job">Complex Network @ Titech</span>
  <span class="author_bio mbm">Our laboratory studies behaviors and structures of complex networks. The official website can be accessed at [net.c.titech.ac.jp]. Check the archive session for a list of all posts.</span>
  <nav class="nav">
    <ul class="nav-list">
      <li class="nav-item">
        <a href="/">home</a>
      </li>
       
      <li class="nav-item">
        <a href="/archive/">Archive</a>
      </li>
          
      <li class="nav-item">
        <a href="/topics/">Topics</a>
      </li>
            
      <li class="nav-item">
        <a href="/tags/">Tags</a>
      </li>
       
    </ul>
  </nav>
  <script type="text/javascript">
  // based on http://stackoverflow.com/a/10300743/280842
  function gen_mail_to_link(hs, subject) {
    var lhs,rhs;
    var p = hs.split('@');
    lhs = p[0];
    rhs = p[1];
    document.write("<a class=\"social-link-item\" target=\"_blank\" href=\"mailto");
    document.write(":" + lhs + "@");
    document.write(rhs + "?subject=" + subject + "\"><i class=\"fa fa-fw fa-envelope\"></i><\/a>");
  }
</script>
<div class="social-links">
  <ul>
    
    
    
    
    
    
    
    
    
    <li><a href="http://github.com/net-titech" class="social-link-item" target="_blank"><i class="fa fa-fw fa-github"></i></a></li>
    
    
    
    
    
    
    
    <li><a href="https://youtube.com/user/net-titech" class="social-link-item" target="_blank"><i class="fa fa-fw fa-youtube"></i></a></li>
    
    
    
    
    
  </ul>
</div>

</div>

      </div>
    </div>
    <div class="col s12 m9">
      <div class="post-listing">
        <a class="btn" href= "/" >
  Home
</a>



<div id="post">
  <header class="post-header">
    <h1 title="Deep Compression Reading List">Deep Compression Reading List</h1>
    <span class="post-meta">
      <span class="post-date">
        27 FEB 2017
      </span>
      •
      <span class="read-time" title="Estimated read time">
  
  
    1 min read
  
</span>

    </span>

  </header>

  <article class="post-content">
    <h2 id="approaches">Approaches</h2>

<p>To compress a neural network, currently there are several approaches:</p>

<ol>
  <li>Stochastic Block Models.</li>
  <li>Tensor Factorization (incl. all decompression techniques).</li>
  <li>Determinantal Point Processes and Submodularity Models.</li>
  <li>Network Distillation and Prunning (incl. fine tuning, etc.)</li>
  <li>Graph Embedding (incl. graph summarization).</li>
  <li>Small or new architectures.</li>
</ol>

<h2 id="i-classfa-fa-calendari-mar-2017"><i class="fa fa-calendar"></i> Mar 2017</h2>

<p class="notice"><strong>Deep Learning without Poor Local Minima</strong>. Kenji Kawaguchi. NIPS. 2016 - <em>Hoang</em></p>
<blockquote>
  <p>This paper is in the top 2% submissions in NIPS 2016.</p>
</blockquote>

<p class="notice"><strong>ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation</strong>. Paszke et al. ArXiv. 2016 - <em>Arie</em></p>
<blockquote>
  <p>We can include ENet architecture for compression comparision.</p>
</blockquote>

<p class="notice"><strong>An Analysis of Deep Neural Networks Model for Practical Applications</strong>. Canziani et al. ArXiv. 2017 - <em>Arie</em></p>

<p class="notice"><strong>Soft Weight-Sharing for Neural Network Compression</strong>. Ullrich et al. ICLR. 2017 - <em>Choong, Kaushalya</em></p>

<p class="notice"><strong>Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</strong>. Han et al. ICLR. 2016 - <em>Choong, Kaushalya</em></p>
<blockquote>
  <p>The best paper award in ICLR 2016. The approach of this paper leverages on pruning, quantization and huffman coding. Source code can be found at <a href="https://github.com/songhan/Deep-Compression-AlexNet">https://github.com/songhan/Deep-Compression-AlexNet</a>. Note that the code only provides ‘Decompression’.</p>
</blockquote>

<p class="notice"><strong>Learning both Weights and Connections for Efficient
Neural Networks</strong>. Han et al. NIPS. 2015 - <em>Choong, Kaushalya</em></p>
<blockquote>
  <p>This paper performs pruning when given a threshold (hyperparamter) on deciding weights that are useful or otherwise. The network is then retrained.</p>
</blockquote>

<p class="notice"><strong>Sparse Convolutional Neural Networks</strong>. Liu et al. CVPR. 2015 - <em>Hoang</em></p>

<h2 id="i-classfa-fa-calendari-feb-2017"><i class="fa fa-calendar"></i> Feb 2017</h2>

<p class="notice"><strong>Distilling the knowledge in a neural network</strong>. Hinton et al. arXiv preprint arXiv:1503.02531. 2015 - <em>Kaushalya</em></p>

  </article>
</div>

<div class="share-buttons">
  <h6>Share on: </h6>
  <ul>
    <li>
      <a href="https://twitter.com/intent/tweet?text=/articles/2017-02/deep-compression" class="twitter btn" title="Share on Twitter"><i class="fa fa-twitter"></i><span> Twitter</span></a>
    </li>
    <li>
      <a href="https://www.facebook.com/sharer/sharer.php?u=/articles/2017-02/deep-compression" class="facebook btn" title="Share on Facebook"><i class="fa fa-facebook"></i><span> Facebook</span></a>
    </li>
    <li>
      <a href="https://plus.google.com/share?url=/articles/2017-02/deep-compression" class="google-plus btn" title="Share on Google Plus"><i class="fa fa-google-plus"></i><span> Google+</span></a>
    </li>
    <li>
      <a href="https://news.ycombinator.com/submitlink?u=/articles/2017-02/deep-compression" class="hacker-news btn" title="Share on Hacker News"><i class="fa fa-hacker-news"></i><span> Hacker News</span></a>
    </li>
    <li>
      <a href="https://www.reddit.com/submit?url=/articles/2017-02/deep-compression" class="reddit btn" title="Share on Reddit"><i class="fa fa-reddit"></i><span> Reddit</span></a>
    </li>
  </ul>
</div><!-- end share-buttons -->



        <footer>
  &copy; 2017 Murata Laboratory. Official website: <a href="www.net.c.titech.ac.jp">www.net.c.titech.ac.jp</a>
</footer>

      </div>
    </div>
  </div>
  <script type="text/javascript" src="/js/jquery-2.1.4.min.js"></script>
<script type="text/javascript" src="/js/main.js"></script>


</body>
</html>
