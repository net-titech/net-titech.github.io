<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Complex Network Research Group (Murata Lab)</title>
    <description>Common resources for Murata Lab members.</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 28 Mar 2017 18:22:31 +0900</pubDate>
    <lastBuildDate>Tue, 28 Mar 2017 18:22:31 +0900</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>Community Detection in Complex Network Reading List</title>
        <description>&lt;h2 id=&quot;i-classfa-fa-calendari-mar-2017&quot;&gt;&lt;i class=&quot;fa fa-calendar&quot;&gt;&lt;/i&gt; Mar 2017&lt;/h2&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;Learning Latent Block Structure in Weighted Networks&lt;/strong&gt;. Christopher Aicher et al. Journal of Complex Networks. 2015 - &lt;em&gt;Choong&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;An stochastic blockmodelling approach for weighted networks. MATLAB implementation can be found at the author’s &lt;a href=&quot;http://tuvalu.santafe.edu/~aaronc/wsbm/&quot;&gt;site&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Tue, 28 Mar 2017 00:00:00 +0900</pubDate>
        <link>/articles/2017-03/community</link>
        <guid isPermaLink="true">/articles/2017-03/community</guid>
        
        
      </item>
    
      <item>
        <title>CREST-Deep Weekly Meeting</title>
        <description>&lt;h1 id=&quot;crest-deep-weekly-meeting&quot;&gt;2017-03-27 / CREST-Deep Weekly Meeting&lt;/h1&gt;

&lt;h2 id=&quot;i-classfa-fa-file-texti-literature-update&quot;&gt;&lt;i class=&quot;fa fa-file-text&quot;&gt;&lt;/i&gt; Literature Update&lt;/h2&gt;

&lt;p&gt;In this week, we have studied several papers regarding techniques and new architectures
for compact deep neural network (especially deep convolutional neural networks).
The list of papers are provided &lt;a href=&quot;https://net-titech.github.io/articles/2017-02/deep-compression&quot;&gt;here&lt;/a&gt;.
We have several observations as follow:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In the area of deep neural network &lt;em&gt;compression&lt;/em&gt;, the standard procedure is: &lt;em&gt;pruning&lt;/em&gt;, &lt;em&gt;quantization&lt;/em&gt;, then &lt;em&gt;Huffman coding&lt;/em&gt;. Although the aforementioned procedure yields good compression rate for &lt;strong&gt;storing&lt;/strong&gt; a deep convolutional neural network (1/40 to 1/60), we still need to &lt;strong&gt;decompress&lt;/strong&gt; a compressed model to its original size in order to run it. None of the proposed models actually run on compressed network. We believe there must be a way to run a compressed network without decompressing it.&lt;/li&gt;
  &lt;li&gt;There are some interesting neural network models applying binary weights, block dropout, and discrete cosine transformation to improve theirs speed (training/running) and compressing the model storage space.&lt;/li&gt;
  &lt;li&gt;Regarding the benchmark criteria for a deep neural network compression model, not many research actually measure the memory requirement of a DNN model in deployment. We think the exact energy consumption and memory usage of &lt;em&gt;each layer&lt;/em&gt; is a very important information to measure the performance of a DNN model.&lt;/li&gt;
  &lt;li&gt;The approach from (Ullrich, 2017) read by Choong is similar to the Stochastic Block Model approach. We can generalize such approach.&lt;/li&gt;
  &lt;li&gt;Investigate the Determinantal Point Processes on graphs gives some interesting suggestions for graph sampling and neural network summarization.&lt;/li&gt;
  &lt;li&gt;SquezeNet is one of the most effective compact deep neural network. It performs particularly well in domain-specific applications. A similar (smaller or higher accuracy) architecture to SquezeNet can be developed.&lt;/li&gt;
  &lt;li&gt;Viewing a neural network as a acyclic (multi-partite) network, we can think of each output element (in an output vector) is the result of computation along many &lt;em&gt;paths&lt;/em&gt;. This view yields some insight about a deep neural network (Kawaguchi, 2016), and also inspired for some other representation of a deep neural network using only one distribution and one matrix.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;i-classfa-fa-flaski-research-update&quot;&gt;&lt;i class=&quot;fa fa-flask&quot;&gt;&lt;/i&gt; Research Update&lt;/h2&gt;

&lt;p&gt;We have run several DNN compression benchmark provided by (Han, 2016) and (Ullrich, 2017) to recreate their results. As mentioned in the previous session, these models compress the network in term of storage space, not running memory space. On the other hand, we have set up Caffe environment on the new machine (titan-x) and downloaded the full size ImageNet.&lt;/p&gt;

&lt;h2 id=&quot;i-classfa-fa-bullseyei-next-week-objectives&quot;&gt;&lt;i class=&quot;fa fa-bullseye&quot;&gt;&lt;/i&gt; Next Week Objectives&lt;/h2&gt;

&lt;p&gt;We have one presentation on April 4th in the meeting with other labs. The content of this presentation is as follow:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Present the current situation of deep model compression. (state of the art algorithms and their limitations)&lt;/li&gt;
  &lt;li&gt;Present our complex network approaches (Filter compression, DPP, SBM, Submodularity, Network summarization).&lt;/li&gt;
  &lt;li&gt;Discuss about the domain-specific constrain.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On the other hand, we will keep exploring and comparing other algorithms and start the benchmark implementation.&lt;/p&gt;
</description>
        <pubDate>Mon, 27 Mar 2017 00:00:00 +0900</pubDate>
        <link>/articles/2017-03/crest-meeting</link>
        <guid isPermaLink="true">/articles/2017-03/crest-meeting</guid>
        
        
      </item>
    
      <item>
        <title>CREST-Deep Team Meeting Protocol</title>
        <description>&lt;h1 id=&quot;crest-deep-meeting&quot;&gt;2017-03-16 / CREST-Deep Meeting&lt;/h1&gt;

&lt;h2 id=&quot;meeting-format&quot;&gt;Meeting format&lt;/h2&gt;

&lt;p&gt;We have decided to have a group meeting &lt;strong&gt;once per week&lt;/strong&gt; to discuss about
new findings and research progress (flexible date). The content of each meeting 
will be recorded as a blog post (like this one). Generally, a meeting has
&lt;strong&gt;three sessions&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Literature update&lt;/em&gt;. Each member has 5 to 10 minutes to talk about papers
they read within the week. We focus on the main idea only. The list of papers
is listed &lt;a href=&quot;https://net-titech.github.io/articles/2017-02/deep-compression&quot;&gt;here&lt;/a&gt;.
Members are also encouraged to write a short blog post for the paper that 
they find interesting.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Reseach update&lt;/em&gt;. We present our difficulty and problems in our research
such as implementation difficulty, new idea, etc.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Discussion&lt;/em&gt;. We spend the last 40 minutes for solution discussion.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Generally, a meeting lasts from 1.5 to 2 hours. It is better for each member
to have a laptop ready in the meeting.&lt;/p&gt;

&lt;h2 id=&quot;project-objective&quot;&gt;Project objective&lt;/h2&gt;

&lt;p&gt;Our target for December 2017 is to develop our graph-theoric deep network
compression technique to achieve Compression Ratio of &lt;em&gt;100&lt;/em&gt;. In this period,
we focus on compressing the Convolutional Neural Network architecture. 
Our top-down milestone:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Dec-Nov 2017: Complete all experiments and comparisions. Finalize paper
drafts and research proposals.&lt;/li&gt;
  &lt;li&gt;Sep-Oct 2017: We have our COMNET benchmark fully functional. For each
CNN architecture (mostly image recognition), we can measure their &lt;strong&gt;accuracy&lt;/strong&gt;,
&lt;strong&gt;running memory on CPU and GPU&lt;/strong&gt;, &lt;strong&gt;storage space on disk&lt;/strong&gt;, &lt;strong&gt;training time&lt;/strong&gt;,
and &lt;strong&gt;prediction time&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Jul-Aug 2017: Implementation of our compressing algorithm.&lt;/li&gt;
  &lt;li&gt;May-Jun 2017: Implementation and data collection for other competing algorithms.&lt;/li&gt;
  &lt;li&gt;April 2017: Literature research and running existing methods.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;benchmark-framework&quot;&gt;Benchmark framework&lt;/h2&gt;

&lt;p&gt;In order to have a robust testing toolbox for the project, we have decided
to make a benchmark framework in which we can compile &lt;strong&gt;Caffe models&lt;/strong&gt;, train
them, and perform various type of measurements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Accuracy: Report accuracy on an isolated testing data. The benchmark should
also report f1-score if the classes distribution is skewed.&lt;/li&gt;
  &lt;li&gt;Memory: Report the memory required to run the deep model. The benchmark shoud
be able to report memory statistics in both running case: CPU and GPU.&lt;/li&gt;
  &lt;li&gt;Time: Report model running time and training time.&lt;/li&gt;
  &lt;li&gt;Storage and power consumption: This feature is optional since we focus on
the memory consumption of a model.&lt;/li&gt;
  &lt;li&gt;Visualization: This feature is optional, check 
&lt;a href=&quot;https://github.com/yosinski/deep-visualization-toolbox&quot;&gt;DeepVis&lt;/a&gt; for more detail.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 16 Mar 2017 00:00:00 +0900</pubDate>
        <link>/articles/2017-03/crest-meeting</link>
        <guid isPermaLink="true">/articles/2017-03/crest-meeting</guid>
        
        
      </item>
    
      <item>
        <title>Introduction to Using Tsubame 2.5</title>
        <description>&lt;p&gt;This post links to the instruction to use Tsubame Supercomputer
written by &lt;a href=&quot;https://github.com/Zepx&quot;&gt;Choong Jun Jin&lt;/a&gt;. 
Currently, the tutorial introduces how to ssh to Tsubame, setup
the proxy for external file download, and git operations. The full wiki
can be accessed &lt;a href=&quot;https://github.com/net-titech/tsubame/wiki&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/net-titech/tsubame/wiki/Getting-Started&quot;&gt;Getting Started&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/net-titech/tsubame/wiki/Accessing-Tsubame&quot;&gt;Accessing Tsubame&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/net-titech/tsubame/wiki/External-Downloads-and-Git-Pulls&quot;&gt;Download External Files and Git Pull&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://github.com/net-titech/tsubame.wiki.git&quot;&gt;Clone this wiki locally!&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Tue, 28 Feb 2017 00:00:00 +0900</pubDate>
        <link>/articles/2017-02/tsubame</link>
        <guid isPermaLink="true">/articles/2017-02/tsubame</guid>
        
        
      </item>
    
      <item>
        <title>Deep Compression Reading List</title>
        <description>&lt;h2 id=&quot;approaches&quot;&gt;Approaches&lt;/h2&gt;

&lt;p&gt;To compress a neural network, currently there are several approaches:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Stochastic Block Models.&lt;/li&gt;
  &lt;li&gt;Tensor Factorization (incl. all decompression techniques).&lt;/li&gt;
  &lt;li&gt;Determinantal Point Processes and Submodularity Models.&lt;/li&gt;
  &lt;li&gt;Network Distillation and Prunning (incl. fine tuning, etc.)&lt;/li&gt;
  &lt;li&gt;Graph Embedding (incl. graph summarization).&lt;/li&gt;
  &lt;li&gt;Small or new architectures.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;i-classfa-fa-calendari-mar-2017&quot;&gt;&lt;i class=&quot;fa fa-calendar&quot;&gt;&lt;/i&gt; Mar 2017&lt;/h2&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;CNNpack: Packing Convolutional Neural Networks in the Frequency Domain&lt;/strong&gt;. Yunhe Wang et al., NIPS. 2016 - &lt;em&gt;Hoang&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;This paper applies the JPEG image compression algorithm to compress the convolutional filters of a deep convolutional neural network. The main difference between this paper and other DNN compression paper is that it uses discrete cosine transformation (DCT) as the pruning procedure (instead of threshold). The comparison in this paper is particularly bad as many other state-of-the-art algorithm are not mentioned. The author claimed 39x compression rate and 25x speed-up rate while maintaining similar top-1 and top-5 accuracy to the original networks (AlexNet). There is no github repository to be found.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;Learning Structured Sparsity in Deep Neural Networks&lt;/strong&gt;. Wen et al., NIPS. 2016 - &lt;em&gt;Kaushalya&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;This paper was &lt;a href=&quot;http://papers.nips.cc/paper/6504-learning-structured-sparsity-in-deep-neural-networks&quot;&gt;presented&lt;/a&gt; a poster at NIPS 2016. This paper introduces group lasso based sparsity regularization to zero out all weights in some structures (filters, channels, and layers) without a significant drop of classification accuracy.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;Deep Learning without Poor Local Minima&lt;/strong&gt;. Kenji Kawaguchi. NIPS. 2016 - &lt;em&gt;Hoang&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The author &lt;a href=&quot;https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Deep-Learning-without-Poor-Local-Minima&quot;&gt;presented&lt;/a&gt; his paper at NIPS 2016. This paper provides a better theoretical understanding about a &lt;em&gt;deep&lt;/em&gt; neural network’s loss surface.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation&lt;/strong&gt;. Paszke et al. ArXiv. 2016 - &lt;em&gt;Arie&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;We can include ENet architecture for compression comparision.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;An Analysis of Deep Neural Networks Model for Practical Applications&lt;/strong&gt;. Canziani et al. ArXiv. 2017 - &lt;em&gt;Arie&lt;/em&gt;&lt;/p&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;Soft Weight-Sharing for Neural Network Compression&lt;/strong&gt;. Ullrich et al. ICLR. 2017 - &lt;em&gt;Choong, Kaushalya&lt;/em&gt;&lt;/p&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding&lt;/strong&gt;. Han et al. ICLR. 2016 - &lt;em&gt;Choong, Kaushalya&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The best paper award in ICLR 2016. The approach of this paper leverages on pruning, quantization and huffman coding. Source code can be found at &lt;a href=&quot;https://github.com/songhan/Deep-Compression-AlexNet&quot;&gt;https://github.com/songhan/Deep-Compression-AlexNet&lt;/a&gt;. Note that the code only provides ‘Decompression’.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;Learning both Weights and Connections for Efficient
Neural Networks&lt;/strong&gt;. Han et al. NIPS. 2015 - &lt;em&gt;Choong, Kaushalya&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;This paper performs pruning when given a threshold (hyperparamter) on deciding weights that are useful or otherwise. The network is then retrained.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;Sparse Convolutional Neural Networks&lt;/strong&gt;. Liu et al. CVPR. 2015 - &lt;em&gt;Hoang&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;i-classfa-fa-calendari-feb-2017&quot;&gt;&lt;i class=&quot;fa fa-calendar&quot;&gt;&lt;/i&gt; Feb 2017&lt;/h2&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;Distilling the knowledge in a neural network&lt;/strong&gt;. Hinton et al. arXiv preprint arXiv:1503.02531. 2015 - &lt;em&gt;Kaushalya&lt;/em&gt;&lt;/p&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;BinaryConnect: Training Deep Neural Networks with binary weights during propagations&lt;/strong&gt;. Courbariaux et al. NIPS 2015 - &lt;em&gt;Arie&lt;/em&gt;&lt;/p&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;Binarized Neural Networks&lt;/strong&gt;. Hubara et al. NIPS 2016 - &lt;em&gt;Arie&lt;/em&gt;&lt;/p&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;Neural Networks with Few Multiplications&lt;/strong&gt;. Lin et al. ICLR 2016 - &lt;em&gt;Arie&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 27 Feb 2017 00:00:00 +0900</pubDate>
        <link>/articles/2017-02/deep-compression</link>
        <guid isPermaLink="true">/articles/2017-02/deep-compression</guid>
        
        
      </item>
    
      <item>
        <title>Network Motif Reading List</title>
        <description>&lt;h2 id=&quot;i-classfa-fa-calendari-jan-2017&quot;&gt;&lt;i class=&quot;fa fa-calendar&quot;&gt;&lt;/i&gt; Jan 2017&lt;/h2&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;Higher-order organization of complex networks&lt;/strong&gt;. Benson et al. Science. 2016 - &lt;em&gt;Hoang&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;More details including datasets, experiments, and discussion about motif Laplacian are provided in the supplementary materials on Science Online.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Sun, 26 Feb 2017 00:00:00 +0900</pubDate>
        <link>/articles/2017-02/motifs-cnet</link>
        <guid isPermaLink="true">/articles/2017-02/motifs-cnet</guid>
        
        
      </item>
    
      <item>
        <title>Submodularity in Complex Network Reading List</title>
        <description>&lt;h2 id=&quot;i-classfa-fa-calendari-mar-2017&quot;&gt;&lt;i class=&quot;fa fa-calendar&quot;&gt;&lt;/i&gt; Mar 2017&lt;/h2&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;Submodular Function Maximization&lt;/strong&gt;. Krause et al. 2012 - &lt;em&gt;Hoang&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;This paper is a survey on maximizing functions with submodularity. The accompanied MATLAB toolbox SFO can be downloaded from &lt;a href=&quot;http://mloss.org/software/view/201/&quot;&gt;http://mloss.org/software/view/201/&lt;/a&gt;. (Hoang: I am interested in implementing the toolbox for Python3.)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;SFO: A Toolbox for Submodular Function Optimization&lt;/strong&gt;. Andreas Krause. JMLR. 2010 - &lt;em&gt;Hoang&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;This short article presents the MATLAB toolbox for submodularity optimization.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;Optimal Approximation for the Submodular Welfare Problem in the Value Oracle Model&lt;/strong&gt;. Jan Vondrak. STOC. 2008 - &lt;em&gt;Hoang&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Vondrak proposed to use expectation as a method to extend the discrete submodular maximization from corners of a matroid to the whole matroid polytope. Named &lt;em&gt;“extension by expectation”&lt;/em&gt;, this new monotone submodular enables us to use a smooth greedy optimization process (real vectors of probabilities instead of discrete membership vectors). This technique can be applied in Submodular Welfare Problem, Separable Assignment Problem, Generalized Assignment Problem, and AdWords Assignment Problem.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Sat, 25 Feb 2017 00:00:00 +0900</pubDate>
        <link>/articles/2017-02/submodularity</link>
        <guid isPermaLink="true">/articles/2017-02/submodularity</guid>
        
        
      </item>
    
  </channel>
</rss>
